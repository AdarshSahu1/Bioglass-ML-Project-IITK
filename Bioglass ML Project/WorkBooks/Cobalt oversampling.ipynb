{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfde4d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\29200\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\29200\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\29200\\anaconda3\\lib\\site-packages (3.0.10)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from pandas) (1.24.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: et_xmlfile in c:\\users\\29200\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas scikit-learn openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "129dde49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset: ['Unnamed: 0', 'RESEARCH PAPER/ ARTICLE ', 'Class', 'SiO2  ', 'B2O3', 'CaO', 'Na2O', 'P2O5', 'K2O', 'MgO', 'Ce', 'Ce2O3', 'CeO2', 'Co', 'CoO', 'Unnamed: 15', 'Conc.', 'Cell Viability 24', 'Cell Viability 48', 'Cell Viability 72', 'Cell Viability 96', 'ALP 7', 'ALP 14', 'Surface area m2/g', 'Pore volume cm3/g', 'Pore size nm', 'Pore to pore distance nm', 'Wall thickness nm', 'VEGF']\n",
      "Columns not found in the dataset: ['SiO2']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\29200\\Downloads\\Dataset\\Cobalt_Data.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Inspect the column names\n",
    "print(\"Columns in the dataset:\", data.columns.tolist())\n",
    "\n",
    "# Define the features and targets\n",
    "features = ['Conc.', 'Cell Viability 24', 'Cell Viability 48', 'Cell Viability 72', 'Cell Viability 96', 'ALP 7', 'ALP 14', 'Surface area m2/g', 'Pore volume cm3/g', 'Pore size nm', 'VEGF']\n",
    "targets = ['SiO2', 'B2O3', 'CaO', 'Na2O', 'P2O5', 'Co', 'CoO']\n",
    "\n",
    "# Check if all specified columns exist in the dataset\n",
    "missing_columns = [col for col in features + targets if col not in data.columns]\n",
    "if missing_columns:\n",
    "    print(f\"Columns not found in the dataset: {missing_columns}\")\n",
    "else:\n",
    "    # Filter the dataset to include only the selected features and target variables\n",
    "    data_filtered = data[features + targets]\n",
    "\n",
    "    # Initialize KNN imputer\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "    # Impute missing values\n",
    "    data_imputed = imputer.fit_transform(data_filtered)\n",
    "\n",
    "    # Convert the imputed data back to a DataFrame\n",
    "    data_imputed = pd.DataFrame(data_imputed, columns=features + targets)\n",
    "\n",
    "    # Ensure no negative values by replacing them with zero\n",
    "    data_imputed[data_imputed < 0] = 0\n",
    "\n",
    "    # Save the imputed dataset to a new Excel file\n",
    "    data_imputed.to_excel('Cobalt_Data_Imputed.xlsx', index=False)\n",
    "\n",
    "    print(\"Imputation completed and saved to 'Cobalt_Data_Imputed.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b115d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the filtered dataset after dropping columns with all missing values: (44, 17)\n",
      "Imputation completed and saved to 'Cobalt_Data_Imputed.xlsx'\n"
     ]
    }
   ],
   "source": [
    "# Check for columns with all missing values and drop them\n",
    "cols_with_all_missing = [col for col in data_filtered.columns if data_filtered[col].isna().all()]\n",
    "if cols_with_all_missing:\n",
    "    print(f\"Columns with all missing values: {cols_with_all_missing}\")\n",
    "    data_filtered = data_filtered.drop(columns=cols_with_all_missing)\n",
    "\n",
    "# Print the shape of the filtered data after dropping columns with all missing values\n",
    "print(\"Shape of the filtered dataset after dropping columns with all missing values:\", data_filtered.shape)\n",
    "\n",
    "# Continue with the imputation and saving steps as before\n",
    "# Initialize KNN imputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Impute missing values\n",
    "data_imputed = imputer.fit_transform(data_filtered)\n",
    "\n",
    "# Convert the imputed data back to a DataFrame\n",
    "data_imputed = pd.DataFrame(data_imputed, columns=data_filtered.columns)\n",
    "\n",
    "# Ensure no negative values by replacing them with zero\n",
    "data_imputed[data_imputed < 0] = 0\n",
    "\n",
    "# Save the imputed dataset to a new Excel file\n",
    "data_imputed.to_excel('Cobalt_Data_Imputed.xlsx', index=False)\n",
    "\n",
    "print(\"Imputation completed and saved to 'Cobalt_Data_Imputed.xlsx'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4481d385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\29200\n",
      "Imputation completed and saved to 'C:\\Users\\29200\\Cobalt_Data_Imputed.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "# Save the imputed dataset to a new Excel file\n",
    "output_file_path = os.path.join(os.getcwd(), 'Cobalt_Data_Imputed.xlsx')\n",
    "data_imputed.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Imputation completed and saved to '{output_file_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4105e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement SMOTERegressor (from versions: none)\n",
      "ERROR: No matching distribution found for SMOTERegressor\n"
     ]
    }
   ],
   "source": [
    "!pip install SMOTERegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f146352b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\29200\\anaconda3\\lib\\site-packages (0.12.3)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\29200\\anaconda3\\lib\\site-packages (from imbalanced-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "049c7e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\29200\\Downloads\\Dataset\\Cobalt_Data_Imputed.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Separate features and target variables\n",
    "features = data[['Conc.', 'Cell Viability 24', 'Cell Viability 48', 'Cell Viability 72', 'ALP 7', 'ALP 14', \n",
    "        'VEGF','Surface area m2/g', 'Pore volume cm3/g', 'Pore size nm']]\n",
    "targets = data[['SiO2', 'B2O3', 'CaO', 'Na2O', 'P2O5', 'Co', 'CoO']]\n",
    "\n",
    "# Random Oversampling\n",
    "X_resampled, y_resampled = resample(features, targets, replace=True, n_samples=len(features) * 3, random_state=42)\n",
    "\n",
    "# Parameters for Gaussian noise\n",
    "noise_level = 0.1  # Adjust the noise level as needed\n",
    "n_samples_to_generate = 50  # Number of new samples to generate\n",
    "\n",
    "# Generate synthetic samples by adding Gaussian noise\n",
    "X_resampled_noise = np.vstack([features.values] + [features.values + noise_level * np.random.normal(size=features.values.shape) for _ in range(n_samples_to_generate)])\n",
    "y_resampled_noise = np.vstack([targets.values] + [targets.values + noise_level * np.random.normal(size=targets.values.shape) for _ in range(n_samples_to_generate)])\n",
    "\n",
    "# Convert resampled data to DataFrame\n",
    "features_resampled = pd.DataFrame(X_resampled, columns=features.columns)\n",
    "targets_resampled = pd.DataFrame(y_resampled, columns=targets.columns)\n",
    "\n",
    "features_noise = pd.DataFrame(X_resampled_noise, columns=features.columns)\n",
    "targets_noise = pd.DataFrame(y_resampled_noise, columns=targets.columns)\n",
    "\n",
    "# Combine features and targets\n",
    "oversampled_resampled = pd.concat([features_resampled, targets_resampled], axis=1)\n",
    "oversampled_noise = pd.concat([features_noise, targets_noise], axis=1)\n",
    "\n",
    "# Save to Excel files\n",
    "oversampled_resampled.to_excel('C:\\\\Users\\\\29200\\\\Downloads\\\\Dataset\\\\Co_oversampled_resampled.xlsx', index=False)\n",
    "oversampled_noise.to_excel('C:\\\\Users\\\\29200\\\\Downloads\\\\Dataset\\\\Co_oversampled_noise.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad26b1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampled Data + before/after Visualisation\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import resample\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\29200\\Downloads\\Dataset\\dropkarungaColumn.xlsx\"\n",
    "data = pd.read_excel(file_path)\n",
    "\n",
    "# Drop columns with too many NaNs and non-numerical columns\n",
    "data_cleaned = data.drop(columns=['RESEARCH PAPER/ ARTICLE', 'VEGF'])\n",
    "\n",
    "# Encode the 'Class' column\n",
    "label_encoder = LabelEncoder()\n",
    "data_cleaned['Class'] = label_encoder.fit_transform(data_cleaned['Class'])\n",
    "\n",
    "# Separate features and target variables\n",
    "#features = data_cleaned.drop(columns=['Cell Viability 24', 'Cell Viability 48', 'Cell Viability 72', 'Cell Viability 96', 'Cell Viability 120', 'ALP 7', 'ALP 14', 'ALP 21'])\n",
    "#targets = data_cleaned[['Cell Viability 24', 'Cell Viability 48', 'Cell Viability 72', 'Cell Viability 96', 'Cell Viability 120', 'ALP 7', 'ALP 14', 'ALP 21']]\n",
    "\n",
    "# Separate features and target variables\n",
    "features = data_cleaned[['Conc.', 'Cell Viability 24', 'Cell Viability 48', 'Cell Viability 72', \n",
    "                          'ALP 7', 'ALP 14',\n",
    "                         'Surface area m2/g', 'Pore volume cm3/g', 'Pore size nm']]\n",
    "targets = data_cleaned[['SiO2', 'B2O3', 'CaO', 'Na2O', 'P2O5', 'Co', 'CoO']]\n",
    "\n",
    "\n",
    "\n",
    "# Random Oversampling\n",
    "X_resampled, y_resampled = resample(features, targets, replace=True, n_samples=len(features) * 2, random_state=42)\n",
    "\n",
    "# Parameters for Gaussian noise\n",
    "noise_level = 0.1  # Adjust the noise level as needed\n",
    "n_samples_to_generate = 500  # Number of new samples to generate\n",
    "\n",
    "# Generate synthetic samples by adding Gaussian noise\n",
    "X_resampled_noise = np.vstack([features.values] + [features.values + noise_level * np.random.normal(size=features.values.shape) for _ in range(n_samples_to_generate)])\n",
    "y_resampled_noise = np.vstack([targets.values] + [targets.values + noise_level * np.random.normal(size=targets.values.shape) for _ in range(n_samples_to_generate)])\n",
    "\n",
    "# Convert resampled data to DataFrame\n",
    "features_resampled = pd.DataFrame(X_resampled, columns=features.columns)\n",
    "targets_resampled = pd.DataFrame(y_resampled, columns=targets.columns)\n",
    "\n",
    "features_noise = pd.DataFrame(X_resampled_noise, columns=features.columns)\n",
    "targets_noise = pd.DataFrame(y_resampled_noise, columns=targets.columns)\n",
    "\n",
    "# Function to plot distributions of the target variables\n",
    "def plot_distributions(targets_before, targets_after, title):\n",
    "    fig, axes = plt.subplots(nrows=targets_before.shape[1], ncols=2, figsize=(15, 20))\n",
    "    for i, column in enumerate(targets_before.columns):\n",
    "        axes[i, 0].hist(targets_before[column], bins=20, alpha=0.7, label='Before')\n",
    "        axes[i, 0].set_title(f'{column} Before Oversampling')\n",
    "        \n",
    "        axes[i, 1].hist(targets_after[column], bins=20, alpha=0.7, label='After', color='orange')\n",
    "        axes[i, 1].set_title(f'{column} After Oversampling')\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the distributions\n",
    "plot_distributions(targets, targets_resampled, \"Distributions Before vs After Random Oversampling\")\n",
    "plot_distributions(targets, targets_noise, \"Distributions Before vs After Gaussian Noise Addition\")\n",
    "\n",
    "# Function to compare summary statistics\n",
    "def compare_statistics(targets_before, targets_after, title):\n",
    "    stats_before = targets_before.describe()\n",
    "    stats_after = targets_after.describe()\n",
    "\n",
    "    comparison = pd.DataFrame({\n",
    "        'Before Mean': stats_before.loc['mean'],\n",
    "        'After Mean': stats_after.loc['mean'],\n",
    "        'Before Std': stats_before.loc['std'],\n",
    "        'After Std': stats_after.loc['std'],\n",
    "        'Before Median': targets_before.median(),\n",
    "        'After Median': targets_after.median(),\n",
    "    })\n",
    "\n",
    "    print(title)\n",
    "    print(comparison)\n",
    "\n",
    "# Compare statistics\n",
    "compare_statistics(targets, targets_resampled, \"Summary Statistics Before vs After Random Oversampling\")\n",
    "compare_statistics(targets, targets_noise, \"Summary Statistics Before vs After Gaussian Noise Addition\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
